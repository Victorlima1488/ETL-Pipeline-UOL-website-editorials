# Web Scraping of UOL Essays (ETL Pipeline)

This project implements an **ETL (Extract, Transform, Load) pipeline** to extract, process, and store essays from **UOL's Essay Bank**, saving the data in **CSV** files for further analysis. It follows the **ETL methodology** to ensure data is collected, cleaned, and structured efficiently.

It uses the following libraries:

- `requests` for HTTP requests  
- `BeautifulSoup` for HTML parsing  
- `csv` for handling extracted data  

---

## ğŸ›  Features  

### ğŸ”¹ **ETL Workflow Overview**  

1. **Extract** â†’ Fetches essays from a predefined list of URLs.  
2. **Transform** â†’ Cleans, structures, and formats the extracted text.  
3. **Load** â†’ Saves the processed data into structured CSV files for analysis.  

### ğŸ”¹ Automatic Collection  
Extracts essays automatically from UOLâ€™s platform using web scraping techniques.  

### ğŸ”¹ Structured Extraction  
Captures key essay information such as:  
- **Title**  
- **Subtitle**  
- **Essay text**  
- **Evaluated competencies**  
- **Scores and comments**  

### ğŸ”¹ Data Transformation  
- **Removes unnecessary spaces** and formats text.  
- **Organizes competencies, scores, and comments** into a structured format.  
- **Ensures consistency in data representation** for further processing.  

### ğŸ”¹ Incremental Data Storage  
- Avoids duplicate processing through tracking mechanisms.  
- Maintains an **ID system** to uniquely identify each essay.  

### ğŸ”¹ Data Persistence (Load)  
Processed data is stored in the following files:  

- **`dados_redacoes.csv`** â†’ Contains all processed essays with their respective details.  
- **`titulos.csv`** â†’ Lists only essay titles and their identifiers.  
- **`processados.txt`** â†’ Logs already processed links to prevent duplication.  
- **`ultimo_id.txt`** â†’ Keeps track of incrementally generated IDs for each essay.  

---

## ğŸš€ Technologies Used  

This project is built with Python and uses the following libraries to implement the **ETL process**:

- **Python 3**  
- `requests` â†’ Handles HTTP requests for extracting data.  
- `BeautifulSoup` â†’ Parses and processes HTML content.  
- `csv` â†’ Structures and stores extracted information.  
- `os` and `re` â†’ Manage files and clean text using regular expressions.  

---

### ğŸ“Œ **Architecture Diagram**  

![ETL Pipeline Architecture](./images/architecture.png)  

---

## â–¶ï¸ How to Run  

1. Clone this repository:  

   ```sh
   git clone https://github.com/your-username/repository-name.git
